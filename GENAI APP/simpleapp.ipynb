{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple GENAI APP using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")  ## Fpr Langsmith tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion: Scrape data from Website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1b7af686200>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/evaluation/tutorials/evaluation\")\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedbackHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to set up a multi-turn evaluationHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a chatbotOn this pageEvaluate a chatbot\\nIn this guide we will set up evaluations for a chatbot.\\nThese allow you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.\\nAt a high level, in this tutorial we will:\\n\\nCreate an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let\\'s dive in!\\nSetup\\u200b\\nFirst install the required dependencies for this tutorial.\\nWe happen to use OpenAI, but LangSmith can be used with any model:\\npip install -U langsmith openai\\nAnd set environment variables to enable LangSmith tracing:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"<Your LangSmith API key>\"export OPENAI_API_KEY=\"<Your OpenAI API key>\"\\nCreate a dataset\\u200b\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?\\n\\nSchema: Each datapoint should consist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that\\'s okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There\\'s no hard and fast rule for how many you should gather.\\nThe main thing is to make sure you have proper coverage of edge cases you may want to guard against.\\nEven 10-50 examples can provide a lot of value!\\nDon\\'t worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand.\\nAfter starting with these datapoints, these datasets are generally living constructs and grow over time.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you\\'ve got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\\nFor this tutorial, we will create 5 datapoints to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet\\'s show how to create and upload this dataset to LangSmith!\\nfrom langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=[        {            \"inputs\": {\"question\": \"What is LangChain?\"},            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is LangSmith?\"},            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is OpenAI?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        },        {            \"inputs\": {\"question\": \"What is Google?\"},            \"outputs\": {\"answer\": \"A technology company known for search\"},        },        {            \"inputs\": {\"question\": \"What is Mistral?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        }    ])\\nNow, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.\\n\\nDefine metrics\\u200b\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\\nThis makes our evaluation a little trickier.\\nIn addition to evaluating correctness, let\\'s also make sure our answers are short and concise.\\nThis will be a little easier - we can define a simple Python function to measure the length of the response.\\nLet\\'s go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\\nWe can define our own prompt and LLM to use for evaluation here:\\nimport openaifrom langsmith import wrappersopenai_client = wrappers.wrap_openai(openai.OpenAI())eval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"\\nFor evaluating the length of the response, this is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\nRun Evaluations\\u200b\\nGreat! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.content\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.\\ndef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}\\nGreat!\\nNow we\\'re ready to run an evaluation.\\nLet\\'s do it!\\nexperiment_results = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)\\nThis will output a URL. If we click on it, we should see results of our evaluation!\\n\\nIf we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\\n\\nLet\\'s now try it out with a different model! Let\\'s try gpt-4-turbo\\ndef ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)\\nAnd now let\\'s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)\\nIf we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!\\n\\nComparing results\\u200b\\nAwesome, we\\'ve evaluated three different runs. But how can we compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.\\nIf we do that, we can see a high level view of the metrics for each run:\\n\\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length.\\nBut what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view.\\nWe immediately see all three tests side by side.\\nSome of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline.\\nWe automatically choose defaults for the baseline and metric, but you can change those yourself.\\nYou can also choose which columns and which metrics you see by using the Display control.\\nYou can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:\\n\\nSet up automated testing to run in CI/CD\\u200b\\nNow that we\\'ve run this in a one-off manner, we can set it to run in an automated fashion.\\nWe can do this pretty easily by just including it as a pytest file that we run in CI/CD.\\nAs part of this, we can either just log the results OR set up some criteria to determine if it passes or not.\\nFor example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,\\nwe could set that up with a test like:\\ndef test_length_score() -> None:    \"\"\"Test that the length score is at least 80%.\"\"\"    experiment_results = evaluate(        ls_target, # Your AI system        data=dataset_name, # The data to predict and grade over        evaluators=[concision, correctness], # The evaluators to score the results    )    # This will be cleaned up in the next release:    feedback = client.list_feedback(        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=\"concision\"    )    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\\nTrack results over time\\u200b\\nNow that we\\'ve got these experiments running in an automated fashion, we want to track these results over time.\\nWe can do this from the overall Experiments tab in the datasets page.\\nBy default, we show evaluation metrics over time (highlighted in red).\\nWe also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\\n\\nConclusion\\u200b\\nThat\\'s it for this tutorial!\\nWe\\'ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time.\\nHopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process.\\nFor example - the datapoints you will want to evaluate on will likely continue to change over time.\\nThere are many types of evaluators you may wish to explore.\\nFor information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this \"offline\" manner (e.g. you can evaluate production data).\\nFor more information on online evaluation, check out this guide.\\nReference code\\u200b\\nClick to see a consolidated code snippetimport openaifrom langsmith import Client, wrappers# Application codeopenai_client = wrappers.wrap_openai(openai.OpenAI())default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.contentclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=[        {            \"inputs\": {\"question\": \"What is LangChain?\"},            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is LangSmith?\"},            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is OpenAI?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        },        {            \"inputs\": {\"question\": \"What is Google?\"},            \"outputs\": {\"answer\": \"A technology company known for search\"},        },        {            \"inputs\": {\"question\": \"What is Mistral?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        }    ])# Define evaluatorseval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"def concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))# Run evaluationsdef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}experiment_results_v1 = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)def ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results_v2 = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)instructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results_v3 = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)Was this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluation tutorialsNextEvaluate a RAG applicationSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs= loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedbackHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='results as a CSVRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to set up a multi-turn evaluationHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a chatbotOn this pageEvaluate a chatbot'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='In this guide we will set up evaluations for a chatbot.\\nThese allow you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.\\nAt a high level, in this tutorial we will:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Create an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let\\'s dive in!\\nSetup\\u200b\\nFirst install the required dependencies for this tutorial.\\nWe happen to use OpenAI, but LangSmith can be used with any model:\\npip install -U langsmith openai\\nAnd set environment variables to enable LangSmith tracing:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"<Your LangSmith API key>\"export OPENAI_API_KEY=\"<Your OpenAI API key>\"\\nCreate a dataset\\u200b\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='What should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Schema: Each datapoint should consist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that's okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There's no hard and fast rule for how many you should gather.\\nThe main thing is to make sure you have proper coverage of edge cases you may want to guard against.\\nEven 10-50 examples can provide a lot of value!\\nDon't worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Even 10-50 examples can provide a lot of value!\\nDon't worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand.\\nAfter starting with these datapoints, these datasets are generally living constructs and grow over time.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you've got your dataset, there are a few different ways to upload them to LangSmith.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"To start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you've got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\\nFor this tutorial, we will create 5 datapoints to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet's show how to create and upload this dataset to LangSmith!\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='from langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=[        {            \"inputs\": {\"question\": \"What is LangChain?\"},            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is LangSmith?\"},            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is OpenAI?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        },        {            \"inputs\": {\"question\": \"What is Google?\"},            \"outputs\": {\"answer\": \"A technology company known for search\"},        },        {            \"inputs\": {\"question\": \"What is Mistral?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='{\"answer\": \"A technology company known for search\"},        },        {            \"inputs\": {\"question\": \"What is Mistral?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        }    ])'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Now, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Define metrics\\u200b\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\\nThis makes our evaluation a little trickier.\\nIn addition to evaluating correctness, let's also make sure our answers are short and concise.\\nThis will be a little easier - we can define a simple Python function to measure the length of the response.\\nLet's go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\\nWe can define our own prompt and LLM to use for evaluation here:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='We can define our own prompt and LLM to use for evaluation here:\\nimport openaifrom langsmith import wrappersopenai_client = wrappers.wrap_openai(openai.OpenAI())eval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"\\nFor evaluating the length of the response, this is a lot easier!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='For evaluating the length of the response, this is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\nRun Evaluations\\u200b\\nGreat! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.content\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.\\ndef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}\\nGreat!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='and then also maps the output of the function to the output key we expect.\\ndef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}\\nGreat!\\nNow we\\'re ready to run an evaluation.\\nLet\\'s do it!\\nexperiment_results = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)\\nThis will output a URL. If we click on it, we should see results of our evaluation!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='If we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Let\\'s now try it out with a different model! Let\\'s try gpt-4-turbo\\ndef ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)\\nAnd now let\\'s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='If we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Comparing results\\u200b\\nAwesome, we've evaluated three different runs. But how can we compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.\\nIf we do that, we can see a high level view of the metrics for each run:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Great! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length.\\nBut what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view.\\nWe immediately see all three tests side by side.\\nSome of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline.\\nWe automatically choose defaults for the baseline and metric, but you can change those yourself.\\nYou can also choose which columns and which metrics you see by using the Display control.\\nYou can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Set up automated testing to run in CI/CD\\u200b\\nNow that we've run this in a one-off manner, we can set it to run in an automated fashion.\\nWe can do this pretty easily by just including it as a pytest file that we run in CI/CD.\\nAs part of this, we can either just log the results OR set up some criteria to determine if it passes or not.\\nFor example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,\\nwe could set that up with a test like:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='For example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,\\nwe could set that up with a test like:\\ndef test_length_score() -> None:    \"\"\"Test that the length score is at least 80%.\"\"\"    experiment_results = evaluate(        ls_target, # Your AI system        data=dataset_name, # The data to predict and grade over        evaluators=[concision, correctness], # The evaluators to score the results    )    # This will be cleaned up in the next release:    feedback = client.list_feedback(        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=\"concision\"    )    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\\nTrack results over time\\u200b\\nNow that we\\'ve got these experiments running in an automated fashion, we want to track these results over time.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content=\"Track results over time\\u200b\\nNow that we've got these experiments running in an automated fashion, we want to track these results over time.\\nWe can do this from the overall Experiments tab in the datasets page.\\nBy default, we show evaluation metrics over time (highlighted in red).\\nWe also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Conclusion\\u200b\\nThat\\'s it for this tutorial!\\nWe\\'ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time.\\nHopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process.\\nFor example - the datapoints you will want to evaluate on will likely continue to change over time.\\nThere are many types of evaluators you may wish to explore.\\nFor information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this \"offline\" manner (e.g. you can evaluate production data).\\nFor more information on online evaluation, check out this guide.\\nReference code\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='Click to see a consolidated code snippetimport openaifrom langsmith import Client, wrappers# Application codeopenai_client = wrappers.wrap_openai(openai.OpenAI())default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.contentclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=[        {            \"inputs\": {\"question\": \"What is LangChain?\"},            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},        },        {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='examples=[        {            \"inputs\": {\"question\": \"What is LangChain?\"},            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is LangSmith?\"},            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},        },        {            \"inputs\": {\"question\": \"What is OpenAI?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        },        {            \"inputs\": {\"question\": \"What is Google?\"},            \"outputs\": {\"answer\": \"A technology company known for search\"},        },        {            \"inputs\": {\"question\": \"What is Mistral?\"},            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},        }    ])# Define evaluatorseval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) ->'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='])# Define evaluatorseval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"def concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))# Run evaluationsdef ls_target(inputs: str) -> dict:    return {\"response\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))# Run evaluationsdef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}experiment_results_v1 = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)def ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results_v2 = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)instructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='= \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results_v3 = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)Was this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluation tutorialsNextEvaluate a RAG applicationSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Transformation: Loaded data-->Docs--> Text into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text to vectors using OPENAI Embeddings \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1b7b7fc99c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Storing vectors in vectorstore DB-FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectordb=FAISS.from_documents(documents=documents,embedding=embeddings)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
