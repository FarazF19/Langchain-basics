{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building important components of Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")  ## Fpr Langsmith tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001DD9F9B2860> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001DD9F9B2F20> root_client=<openai.OpenAI object at 0x000001DD9B9AA080> root_async_client=<openai.AsyncOpenAI object at 0x000001DD9F9B2890> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and response from LLM\n",
    "result=llm.invoke(\"What is Generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a class of artificial intelligence systems designed to generate new content. These systems use machine learning models, particularly deep learning techniques, to create data that is similar to a particular training set. Examples of generative AI include text, images, audio, or even video. \\n\\nSome key characteristics and applications of generative AI include:\\n\\n1. **Text Generation**: Models like OpenAI's GPT (Generative Pre-trained Transformer) are designed to generate human-like text. They can be used for text completion, translation, summarization, and more.\\n\\n2. **Image Generation**: Tools such as GANs (Generative Adversarial Networks) can create realistic images from scratch, and models like DALL-E and Stable Diffusion can generate images from textual descriptions.\\n\\n3. **Audio and Music**: Generative AI can be used to produce music, synthesize realistic speech, or create entirely new audio experiences.\\n\\n4. **Video and Animation**: AI technologies are advancing to create realistic video content, animate still images, or even produce deepfake videos, where people appear to say or do things they haven't.\\n\\n5. **Design and Art**: Artists and designers use generative AI to help create new styles or pieces of work, harnessing AI as a creative partner.\\n\\nThe underlying technology often involves neural networks, which are trained on large datasets to learn the patterns within the data. Once trained, these models can then generate new outputs that are similar in style and structure to the training data but are entirely new creations. This capability has myriad applications across industries, from entertainment and gaming to marketing and beyond.\\n\\nGenerative AI raises various ethical questions, particularly related to content authenticity, copyright, and the potential for misuse in spreading misinformation. As such, the development and deployment of generative AI technologies are accompanied by discussions around ethical standards and regulations.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 374, 'prompt_tokens': 13, 'total_tokens': 387, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_898ac29719', 'id': 'chatcmpl-BKPh9imRW0G9y1rUTt8XmthcY5yWs', 'finish_reason': 'stop', 'logprobs': None} id='run-3a476eb2-20fe-49f3-be10-ca0ce0e382d1-0' usage_metadata={'input_tokens': 13, 'output_tokens': 374, 'total_tokens': 387, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are expert AI Engineer,Provide me answers based on the questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt Templates\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are expert AI Engineer,Provide me answers based on the questions.\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    " \n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langchain Expression Language (LCEL) is a feature of the LangChain framework that allows you to perform complex operations and manipulations directly within your language models. Think of it like a toolset that you can use to perform functions such as formatting data, conducting arithmetic operations, or even managing control flows, all while interacting with language models.\\n\\nIn simple terms, LCEL enables you to incorporate logical and computational expressions into your prompts. This means you can execute small code snippets to transform or evaluate data as you work with your language model. It helps in creating more dynamic and programmable prompts which can lead to more efficient and powerful workflows.\\n\\nFor example, if you're using LangChain to build an application that needs to retrieve specific types of information or make decisions based on certain criteria, LCEL can help you do this seamlessly within the prompt configuration, rather than needing to handle such logic externally in your application code.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 181, 'prompt_tokens': 43, 'total_tokens': 224, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BKPhLAxcCPoqNntbNr1jwygtfXQMq', 'finish_reason': 'stop', 'logprobs': None}, id='run-1728e4f6-effc-462a-88c8-1f721adbe6ec-0', usage_metadata={'input_tokens': 43, 'output_tokens': 181, 'total_tokens': 224, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chain\n",
    "chain= prompt|llm\n",
    "response=chain.invoke({\"input\":\"Tell me about Langchain Expression Langaugage(LCEL) in an easy way.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain Expression Language (LCEL) is a component of the Langchain framework designed to make it easier for developers to work with and manage language models. LCEL provides a simplified and structured way to define and manipulate expressions that interact with language models. This involves constructing complex tasks using natural language processing (NLP) capabilities, such as text generation or information retrieval, in a more intuitive and human-readable format.\\n\\nIn essence, LCEL acts as a middleware between developers and complex language models, allowing the creation of powerful applications without needing deep expertise in NLP algorithms. It can be used to chain together different components or prompts efficiently, making the process of constructing queries, processing data, and generating responses smoother and more streamlined.\\n\\nThink of LCEL as a scripting or query language tailored specifically for optimizing interactions with AI language models, enabling rapid development and deployment of language-based applications.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## String Output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain= prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"Tell me about Langchain Expression Langaugage(LCEL) in an easy way.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
